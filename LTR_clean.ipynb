{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66e4403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Loading Data\n",
      "\n",
      "üîπ Processing Data\n",
      "\n",
      "üîπ Creating LTR Dataset\n",
      "\n",
      "üîπ Splitting Data\n",
      "\n",
      "üîπ Preparing Features\n",
      "\n",
      "üîπ Training Model\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[250]\tvalid_0's ndcg@1: 0.914476\tvalid_0's ndcg@3: 0.914816\tvalid_0's ndcg@5: 0.928875\tvalid_0's ndcg@10: 0.946661\n",
      "\n",
      "üîπ Evaluating NDCG Scores\n",
      "\n",
      "‚úÖ FINAL TEST RESULTS\n",
      "==============================\n",
      "NDCG_1  : 0.9493 (94.93%)\n",
      "NDCG_3  : 0.9346 (93.46%)\n",
      "NDCG_5  : 0.9458 (94.58%)\n",
      "NDCG_10 : 0.9590 (95.90%)\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Core LTR Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score\n",
    "import lightgbm as lgb\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def minimal_log_step(title):\n",
    "    print(f\"\\nüîπ {title}\")\n",
    "\n",
    "def show_final_test_results(ndcg_results):\n",
    "    print(\"\\n‚úÖ FINAL TEST RESULTS\")\n",
    "    print(\"=\" * 30)\n",
    "    for key, val in ndcg_results['avg_ndcg_scores'].items():\n",
    "        print(f\"{key.upper():<8}: {val:.4f} ({val * 100:.2f}%)\")\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "# Load and merge data\n",
    "def load_data():\n",
    "    file1 = pd.read_excel(\"Simay_1.xlsx\")\n",
    "    file2 = pd.read_excel(\"Simay_2.xlsx\")\n",
    "\n",
    "    '''\n",
    "    NEW DATA LOADING EXAMPLE\n",
    "\n",
    "    new_file = pd.read_excel(\"new_data.xlsx\") # write your file's name instead of new_data.xlsx\n",
    "    file2 = pd.concat([file2, new_file], ignore_index=True)\n",
    "\n",
    "    '''\n",
    "    return pd.merge(file2, file1, on='CD_ORDERTYPE', how='left')\n",
    "\n",
    "# Feature engineering - encoding and datetime processing\n",
    "def process_data(df):\n",
    "    df['DT_ORDER'] = pd.to_datetime(df['DT_ORDER'])\n",
    "    df['SW_URGENTORDER'] = df['SW_URGENTORDER'].fillna(0)\n",
    "    df['SW_CB_URGENTORDER'] = df['SW_CB_URGENTORDER'].fillna(0)\n",
    "    df['IS_URGENT'] = ((df['SW_URGENTORDER'] == 1) | (df['SW_CB_URGENTORDER'] == 1)).astype(int)\n",
    "\n",
    "    max_p = df['NO_PRIORITY'].max()\n",
    "    df['FINAL_PRIORITY'] = df['IS_URGENT'] * 1000 + (max_p - df['NO_PRIORITY'] + 1)\n",
    "\n",
    "    df['HOUR_OF_DAY'] = df['DT_ORDER'].dt.hour\n",
    "    df['DAY_OF_WEEK'] = df['DT_ORDER'].dt.dayofweek\n",
    "    df['ORDER_AGE_HOURS'] = (datetime.now() - df['DT_ORDER']).dt.total_seconds() / 3600\n",
    "\n",
    "    le_dealer = LabelEncoder()\n",
    "    le_ordertype = LabelEncoder()\n",
    "    df['CD_DEALER_ENCODED'] = le_dealer.fit_transform(df['CD_DEALER'].astype(str))\n",
    "    df['CD_ORDERTYPE_ENCODED'] = le_ordertype.fit_transform(df['CD_ORDERTYPE'].astype(str))\n",
    "\n",
    "    return df, le_dealer, le_ordertype\n",
    "\n",
    "# create_ltr_dataset function with business-driven scoring\n",
    "def create_business_driven_ltr_dataset(df, min_group_size=4, max_group_size=50):\n",
    "    grouped_data = []\n",
    "\n",
    "    def normalize(col):\n",
    "        col_min, col_max = col.min(), col.max()\n",
    "        if col_max > col_min:\n",
    "            return (col - col_min) / (col_max - col_min)\n",
    "        else:\n",
    "            return pd.Series(0.5, index=col.index)  # maintain index\n",
    "\n",
    "    for part_id, group in df.groupby('CD_PART'):\n",
    "        if len(group) < min_group_size or len(group) > max_group_size:\n",
    "            continue\n",
    "\n",
    "        group = group.copy()\n",
    "\n",
    "        norm_priority = 1 - normalize(group['NO_PRIORITY'])\n",
    "        norm_age = normalize(group['ORDER_AGE_HOURS'])\n",
    "        norm_qty = normalize(group['QT_ORDER'])\n",
    "        norm_hour = normalize(group['HOUR_OF_DAY'])\n",
    "\n",
    "        noise = pd.Series(np.random.normal(0, 0.01, len(group)), index=group.index)\n",
    "\n",
    "        group['BUSINESS_SCORE'] = (\n",
    "            norm_priority * 0.4 +\n",
    "            norm_age * 0.3 +\n",
    "            norm_qty * 0.2 +\n",
    "            norm_hour * 0.1 +\n",
    "            noise\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            group['RELEVANCE_LABEL'] = pd.qcut(\n",
    "                group['BUSINESS_SCORE'],\n",
    "                q=5,\n",
    "                labels=False,\n",
    "                duplicates='drop'\n",
    "            ).astype(int)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        if group['RELEVANCE_LABEL'].nunique() < 2:\n",
    "            continue\n",
    "\n",
    "        group['GROUP_ID'] = f\"part_{part_id}\"\n",
    "        grouped_data.append(group)\n",
    "\n",
    "    if not grouped_data:\n",
    "        return None, None\n",
    "\n",
    "    final_df = pd.concat(grouped_data, ignore_index=True)\n",
    "    group_sizes = final_df.groupby('GROUP_ID').size().tolist()\n",
    "\n",
    "    return final_df, group_sizes\n",
    "\n",
    "'''\n",
    "# Create robust LTR dataset\n",
    "def create_ltr_dataset(df, min_group_size=4, max_group_size=50):\n",
    "    groups = []\n",
    "    for part_id, group in df.groupby('CD_PART'):\n",
    "        if len(group) < min_group_size or len(group) > max_group_size:\n",
    "            continue\n",
    "        if group['FINAL_PRIORITY'].nunique() < 2:\n",
    "            continue\n",
    "        group = group.sort_values('FINAL_PRIORITY', ascending=False).copy()\n",
    "        group['RELEVANCE_LABEL'] = pd.qcut(\n",
    "            group['FINAL_PRIORITY'] + np.random.normal(0, 0.001, len(group)),\n",
    "            q=min(5, len(group)),\n",
    "            labels=False,\n",
    "            duplicates='drop'\n",
    "        )\n",
    "        group['RELEVANCE_LABEL'] = group['RELEVANCE_LABEL'].max() - group['RELEVANCE_LABEL']\n",
    "        group['GROUP_ID'] = f\"part_{part_id}\"\n",
    "        if group['RELEVANCE_LABEL'].nunique() >= 2:\n",
    "            groups.append(group)\n",
    "\n",
    "    if not groups:\n",
    "        return None, None\n",
    "    final_df = pd.concat(groups, ignore_index=True)\n",
    "    group_sizes = final_df.groupby('GROUP_ID').size().tolist()\n",
    "    return final_df, group_sizes\n",
    "'''\n",
    "\n",
    "#EXCLUDE IS_URGENT from features\n",
    "def prepare_features(df):\n",
    "    cols = [\n",
    "        'QT_ORDER', 'NO_PRIORITY',\n",
    "        'HOUR_OF_DAY', 'DAY_OF_WEEK', 'ORDER_AGE_HOURS',\n",
    "        'CD_DEALER_ENCODED', 'CD_ORDERTYPE_ENCODED'\n",
    "    ]\n",
    "    X = df[cols].copy()\n",
    "    y = df['RELEVANCE_LABEL']\n",
    "\n",
    "    num_cols = ['QT_ORDER', 'ORDER_AGE_HOURS', 'NO_PRIORITY']\n",
    "    X[num_cols] = X[num_cols].astype(float)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "\n",
    "    return X, y, cols, scaler\n",
    "\n",
    "'''\n",
    "# Feature prep\n",
    "def prepare_features(df):\n",
    "    cols = ['QT_ORDER', 'NO_PRIORITY', 'IS_URGENT',\n",
    "            'HOUR_OF_DAY', 'DAY_OF_WEEK', 'ORDER_AGE_HOURS',\n",
    "            'CD_DEALER_ENCODED', 'CD_ORDERTYPE_ENCODED']\n",
    "    X = df[cols].copy()\n",
    "    y = df['RELEVANCE_LABEL']\n",
    "    num_cols = ['QT_ORDER', 'ORDER_AGE_HOURS', 'NO_PRIORITY']\n",
    "    X[num_cols] = X[num_cols].astype(float)\n",
    "    scaler = StandardScaler()\n",
    "    X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "    return X, y, cols, scaler\n",
    "'''\n",
    "# Split groups into train, validation, and test sets -not splitting orders-\n",
    "def split_data(df, group_sizes, test_size=0.2, val_size=0.2):\n",
    "    groups = df['GROUP_ID'].unique()\n",
    "    train_g, temp_g = train_test_split(groups, test_size=test_size + val_size, random_state=42)\n",
    "    val_g, test_g = train_test_split(temp_g, test_size=test_size / (test_size + val_size), random_state=42)\n",
    "\n",
    "    train_df = df[df['GROUP_ID'].isin(train_g)]\n",
    "    val_df = df[df['GROUP_ID'].isin(val_g)]\n",
    "    test_df = df[df['GROUP_ID'].isin(test_g)]\n",
    "\n",
    "    return (train_df, val_df, test_df), (\n",
    "        train_df.groupby('GROUP_ID').size().tolist(),\n",
    "        val_df.groupby('GROUP_ID').size().tolist(),\n",
    "        test_df.groupby('GROUP_ID').size().tolist()\n",
    "    )\n",
    "\n",
    "# LightGBM training\n",
    "def train_model(X_train, y_train, train_groups, X_val, y_val, val_groups):\n",
    "    train_data = lgb.Dataset(X_train, label=y_train, group=train_groups)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, group=val_groups, reference=train_data)\n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'lambda_l1': 1.0,\n",
    "        'lambda_l2': 1.0,\n",
    "        'min_data_in_leaf': 10,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'max_depth': 6,\n",
    "        'verbose': -1,\n",
    "        'ndcg_eval_at': [1, 3, 5, 10],\n",
    "    }\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        valid_sets=[val_data],\n",
    "        num_boost_round=1000,\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# NDCG evaluation\n",
    "def evaluate_model(model, X_test, y_test, test_groups):\n",
    "    y_pred = model.predict(X_test)\n",
    "    start = 0\n",
    "    scores = {'ndcg_1': [], 'ndcg_3': [], 'ndcg_5': [], 'ndcg_10': []}\n",
    "    for size in test_groups:\n",
    "        end = start + size\n",
    "        y_true = np.array([y_test[start:end]])\n",
    "        y_score = np.array([y_pred[start:end]])\n",
    "        scores['ndcg_1'].append(ndcg_score(y_true, y_score, k=1))\n",
    "        scores['ndcg_3'].append(ndcg_score(y_true, y_score, k=3))\n",
    "        scores['ndcg_5'].append(ndcg_score(y_true, y_score, k=5))\n",
    "        scores['ndcg_10'].append(ndcg_score(y_true, y_score, k=min(10, size)))\n",
    "        start = end\n",
    "    avg_scores = {k: np.mean(v) for k, v in scores.items()}\n",
    "    return {'avg_ndcg_scores': avg_scores}\n",
    "\n",
    "# Part 2: Detailed Analysis Module\n",
    "def run_detailed_analyses(df, ltr_df, model, feature_columns, le_dealer, le_ordertype, X_test, y_test, test_groups):\n",
    "    print(\"\\nüìä RUNNING DETAILED ANALYSES\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # 1. Feature Importance\n",
    "    print(\"\\nüîç Feature Importance\")\n",
    "    importance_scores = model.feature_importance(importance_type='gain')\n",
    "    feature_mapping = {\n",
    "        'QT_ORDER': 'Order Quantity',\n",
    "        'NO_PRIORITY': 'Rule-Based Priority Level',\n",
    "        'IS_URGENT': 'Urgent Flag',\n",
    "        'HOUR_OF_DAY': 'Hour of Day',\n",
    "        'DAY_OF_WEEK': 'Day of Week',\n",
    "        'ORDER_AGE_HOURS': 'Order Age',\n",
    "        'CD_DEALER_ENCODED': 'Dealer Type',\n",
    "        'CD_ORDERTYPE_ENCODED': 'Order Type'\n",
    "    }\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'technical': feature_columns,\n",
    "        'label': [feature_mapping.get(col, col) for col in feature_columns],\n",
    "        'importance': importance_scores\n",
    "    })\n",
    "    feature_importance_df['%'] = (\n",
    "        feature_importance_df['importance'] / feature_importance_df['importance'].sum()\n",
    "    ) * 100\n",
    "    feature_importance_df = feature_importance_df.sort_values('%', ascending=False)\n",
    "\n",
    "    for _, row in feature_importance_df.iterrows():\n",
    "        print(f\"‚Ä¢ {row['label']:<22}: {row['%']:.2f}%\")\n",
    "\n",
    "    # 2. Rule-based vs. LTR ranking\n",
    "    print(\"\\n‚öñÔ∏è  Rule-Based vs. LTR Ranking Differences\")\n",
    "    changes = []\n",
    "    for part in ltr_df['CD_PART'].unique():\n",
    "        part_df = ltr_df[ltr_df['CD_PART'] == part].copy()\n",
    "        if len(part_df) < 2:\n",
    "            continue\n",
    "        part_df = part_df.sort_values(\"FINAL_PRIORITY\", ascending=False).reset_index(drop=True)\n",
    "        part_df['RULE_RANK'] = range(1, len(part_df)+1)\n",
    "\n",
    "        X_part = part_df[feature_columns]\n",
    "        part_df['LTR_SCORE'] = model.predict(X_part)\n",
    "        part_df = part_df.sort_values(\"LTR_SCORE\", ascending=False).reset_index(drop=True)\n",
    "        part_df['LTR_RANK'] = range(1, len(part_df)+1)\n",
    "\n",
    "        part_df['DIFF'] = part_df['RULE_RANK'] - part_df['LTR_RANK']\n",
    "        if part_df['DIFF'].abs().sum() > 0:\n",
    "            changes.append(part_df['DIFF'].abs().mean())\n",
    "\n",
    "    if changes:\n",
    "        print(f\"‚Ä¢ Average rank difference in changed parts: {np.mean(changes):.2f}\")\n",
    "        print(f\"‚Ä¢ % of parts with changed rankings        : {len(changes)/ltr_df['CD_PART'].nunique()*100:.1f}%\")\n",
    "    else:\n",
    "        print(\"‚Ä¢ No significant differences between rule-based and LTR rankings.\")\n",
    "\n",
    "    # 3. Temporal Dynamics\n",
    "    print(\"\\n‚è≥ Temporal Priority Dynamics\")\n",
    "    df['ORDER_MONTH'] = df['DT_ORDER'].dt.month\n",
    "    urgency_by_month = df.groupby('ORDER_MONTH')['IS_URGENT'].mean() * 100\n",
    "    for month, pct in urgency_by_month.items():\n",
    "        print(f\"‚Ä¢ Month {month:>2}: {pct:5.1f}% urgent orders\")\n",
    "\n",
    "    # 4. Executive Summary\n",
    "    print(\"\\nüìã EXECUTIVE SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    top_feat = feature_importance_df.iloc[0]\n",
    "    print(f\"‚Ä¢ Top feature driving decisions: {top_feat['label']} ({top_feat['%']:.1f}%)\")\n",
    "    if changes:\n",
    "        print(f\"‚Ä¢ {len(changes)} parts showed altered rankings by LTR\")\n",
    "    else:\n",
    "        print(f\"‚Ä¢ LTR aligns closely with rule-based for all parts\")\n",
    "    print(f\"‚Ä¢ Urgency peaks in month: {urgency_by_month.idxmax()} ({urgency_by_month.max():.1f}%)\")\n",
    "\n",
    "    return {\n",
    "        'feature_importance': feature_importance_df,\n",
    "        'avg_rank_diff': np.mean(changes) if changes else 0,\n",
    "        'urgency_by_month': urgency_by_month\n",
    "    }\n",
    "\n",
    "# Entry point\n",
    "def main_pipeline(run_analysis=False):\n",
    "    minimal_log_step(\"Loading Data\")\n",
    "    df = load_data()\n",
    "\n",
    "    minimal_log_step(\"Processing Data\")\n",
    "    df, le_dealer, le_ordertype = process_data(df)\n",
    "\n",
    "    minimal_log_step(\"Creating LTR Dataset\")\n",
    "    ltr_df, group_sizes = create_business_driven_ltr_dataset(df)\n",
    "    #ltr_df, group_sizes = create_ltr_dataset(df)\n",
    "\n",
    "    if ltr_df is None:\n",
    "        print(\"‚ùå No valid groups found for LTR training.\")\n",
    "        return\n",
    "\n",
    "    minimal_log_step(\"Splitting Data\")\n",
    "    (train_df, val_df, test_df), (train_groups, val_groups, test_groups) = split_data(ltr_df, group_sizes)\n",
    "\n",
    "    minimal_log_step(\"Preparing Features\")\n",
    "    X_train, y_train, feature_columns, scaler = prepare_features(train_df)\n",
    "    X_val, y_val, _, _ = prepare_features(val_df)\n",
    "    X_test, y_test, _, _ = prepare_features(test_df)\n",
    "\n",
    "    minimal_log_step(\"Training Model\")\n",
    "    model = train_model(X_train, y_train, train_groups, X_val, y_val, val_groups)\n",
    "\n",
    "    minimal_log_step(\"Evaluating NDCG Scores\")\n",
    "    ndcg_results = evaluate_model(model, X_test, y_test, test_groups)\n",
    "    show_final_test_results(ndcg_results)\n",
    "\n",
    "    return {\n",
    "        'df': df,\n",
    "        'ltr_df': ltr_df,\n",
    "        'model': model,\n",
    "        'feature_columns': feature_columns,\n",
    "        'le_dealer': le_dealer,\n",
    "        'le_ordertype': le_ordertype,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'test_groups': test_groups\n",
    "    } if run_analysis else None\n",
    "\n",
    "result = main_pipeline(run_analysis=False)\n",
    "if result:\n",
    "    run_detailed_analyses(\n",
    "        df=result['df'],\n",
    "        ltr_df=result['ltr_df'],\n",
    "        model=result['model'],\n",
    "        feature_columns=result['feature_columns'],\n",
    "        le_dealer=result['le_dealer'],\n",
    "        le_ordertype=result['le_ordertype'],\n",
    "        X_test=result['X_test'],\n",
    "        y_test=result['y_test'],\n",
    "        test_groups=result['test_groups']\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
